\documentclass[letterpaper,11pt]{article}
\usepackage{listings}
\usepackage[pdftex]{graphicx} 
\usepackage[utf8]{inputenc}
%\usepackage[english]{babel}
\usepackage{alltt}
\usepackage{color}
\usepackage{url}
\usepackage[T1]{fontenc}
\usepackage{float}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{caption}
\usepackage{spverbatim}
\usepackage[table,xcdraw]{xcolor}
\usepackage{multirow}
\usepackage{amsmath}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\addtolength{\textwidth}{4cm}
\addtolength{\hoffset}{-2cm}
\addtolength{\textheight}{4cm}
\addtolength{\voffset}{-2cm}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\lstset{
	basicstyle=\footnotesize,
	breaklines=true,
}

\title{Bibliography management: BibTeX}
\author{Share\LaTeX}

\begin{document}

\begin{titlepage}

\begin{center}

\Huge{Assignment 5}

\Large{CS834-F16:  Introduction to Information Retrieval}

\Large{Fall 2016}


\Large{Erika Siregar}

\vfill

% Bottom of the page
\Large{CS Department - Old Dominion University  \\ \today}


\end{center}

\end{titlepage}


\section*{Question 10.3}
\begin{spverbatim}
Compute five iterations of HITS (see Algorithm 3) and PageRank (see Figure 4.11) on the graph in Figure 10.3. Discuss how the PageRank scores compare to the hub and authority scores produced by HITS.
\end{spverbatim}

\subsection*{Answer}

Figure \ref{fig:10_3} shows the directed graph from the textbook \cite{Croft:2009:SEI:1516224} on which we will calculate the scores of HITS and PageRank. Computing HITS (authorities and hubs) and PageRank scores are pretty easy since we can just utilize the Link Analysis procedure that is provided by python library `networkx' \cite{networkx-hits-pagerank}. 

\begin{figure}[H]
	\fbox{\includegraphics[scale=0.6]{10_3}}
	\centering
	\caption{Figure 10.3 from the textbook \cite{Croft:2009:SEI:1516224}}
	\label{fig:10_3}
\end{figure}

Figure \ref{fig:hits_pagerank} shows the scores of HITS (authorities and hubs) and PageRank, which are obtained by running the code in listing \ref{lst:10_3.py}. We only need to set the number of iterations. 

\begin{figure}[H]
	\fbox{\includegraphics[scale=0.6]{hits_pagerank}}
	\centering
	\caption{HITS and Pagerank for Figure 10.3 with 5 Iterations}
	\label{fig:hits_pagerank}
\end{figure}

To make the analysis and comparison easier, I transformed the output int figure \ref{fig:hits_pagerank} into a neat table format as can be seen on table \ref{tab: hits_pagerank}. From table \ref{tab: hits_pagerank}, we can see that, generally, the authorities values are linearly proportional to those of PageRank. After 5 iterations, node 3 gets the highest score for `authorities' and the second highest score for `PageRank'. Nodes 1 and 2 get lower `authorities' score than that of node 3, but higher `authorities' score compare to nodes 5 and 6. The same thing can also be concluded by comparing the PageRank scores for those five nodes (1, 2, 3, 5, and 6). The strange thing happens on node 4, where its `authorities' score is lower than node 3, but its `PageRank' score is higher than node 3. This anomaly takes place probably because we only do 5 iterations. Maybe, if we continue iterating until the values converge into certain number, this anomaly will not happen. 

\begin{table}[h!]
	\centering
	\begin{tabular}{|l|l|l|l|}
		\hline
		\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{Node}}} & \multicolumn{3}{c|}{\textbf{Score}} \\ \cline{2-4} 
		\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{\textbf{Hubs}} & \multicolumn{1}{c|}{\textbf{Authorities}} & \multicolumn{1}{c|}{\textbf{PageRank}} \\ \hline
		1 & 0.198707510685935 & 0.201534170153416 & 0.154152105903131 \\ \hline
		2 & 0.198707510685935 & 0.201534170153416 & 0.154152105903131 \\ \hline
		3 & 0.258854060655404 & 0.252324500232450 & 0.215183655595341 \\ \hline
		4 & 0.178963973132505 & 0.188168293816829 & 0.273070054356128 \\ \hline
		5 & 0.082383472420110 & 0.078219432821943 & 0.089524353405041 \\ \hline
		6 & 0.082383472420110 & 0.078219432821943 & 0.089524353405041 \\ \hline
		7 & 0.000000000000000 & 0.000000000000000 & 0.024393371432184 \\ \hline
	\end{tabular}
	\caption{HITS and Pagerank for Figure 10.3 with 5 Iterations}
	\label{tab: hits_pagerank}
\end{table}

\begin{lstlisting}[language=python, caption={Computing HITS and PageRank}, label={lst:10_3.py}]
#!/usr/bin/python

import networkx as nx

def hits(G, iter=100, nstart=None, normalized=True):
if type(G) == nx.MultiGraph or type(G) == nx.MultiDiGraph:
raise Exception("hits() not defined for graphs with multiedges.")
if len(G) == 0:
return {},{}
# choose fixed starting vector if not given
if nstart is None:
h=dict.fromkeys(G,1.0/G.number_of_nodes())
else:
h=nstart
# normalize starting vector
s=1.0/sum(h.values())
for k in h:
h[k]*=s
i=0
while True: # power iteration: make up to max_iter iterations
if i >= iter: break

hlast=h
h=dict.fromkeys(hlast.keys(),0)
a=dict.fromkeys(hlast.keys(),0)
# this "matrix multiply" looks odd because it is
# doing a left multiply a^T=hlast^T*G
for n in h:
for nbr in G[n]:
a[nbr]+=hlast[n]*G[n][nbr].get('weight',1)
# now multiply h=Ga
for n in h:
for nbr in G[n]:
h[n]+=a[nbr]*G[n][nbr].get('weight',1)
# normalize vector
s=1.0/max(h.values())
for n in h: h[n]*=s
# normalize vector
s=1.0/max(a.values())
for n in a: a[n]*=s

i+=1
if normalized:
s = 1.0/sum(a.values())
for n in a:
a[n] *= s
s = 1.0/sum(h.values())
for n in h:
h[n] *= s
return h,a

def pagerank(G, alpha=0.85, personalization=None,
iter=100, nstart=None, weight='weight',
dangling=None):
if len(G) == 0:
return {}

if not G.is_directed():
D = G.to_directed()
else:
D = G

# Create a copy in (right) stochastic form
W = nx.stochastic_graph(D, weight=weight)
N = W.number_of_nodes()

# Choose fixed starting vector if not given
if nstart is None:
x = dict.fromkeys(W, 1.0 / N)
else:
# Normalized nstart vector
s = float(sum(nstart.values()))
x = dict((k, v / s) for k, v in nstart.items())

if personalization is None:
# Assign uniform personalization vector if not given
p = dict.fromkeys(W, 1.0 / N)
else:
missing = set(G) - set(personalization)
if missing:
raise nx.NetworkXError('Personalization dictionary '
'must have a value for every node. '
'Missing nodes %s' % missing)
s = float(sum(personalization.values()))
p = dict((k, v / s) for k, v in personalization.items())

if dangling is None:
# Use personalization vector if dangling vector not specified
dangling_weights = p
else:
missing = set(G) - set(dangling)
if missing:
raise nx.NetworkXError('Dangling node dictionary '
'must have a value for every node. '
'Missing nodes %s' % missing)
s = float(sum(dangling.values()))
dangling_weights = dict((k, v/s) for k, v in dangling.items())
dangling_nodes = [n for n in W if W.out_degree(n, weight=weight) == 0.0]

# power iteration: make up to max_iter iterations
for _ in range(iter):
xlast = x
x = dict.fromkeys(xlast.keys(), 0)
danglesum = alpha * sum(xlast[n] for n in dangling_nodes)
for n in x:
# this matrix multiply looks odd because it is
# doing a left multiply x^T=xlast^T*W
for nbr in W[n]:
x[nbr] += alpha * xlast[n] * W[n][nbr][weight]
x[n] += danglesum * dangling_weights[n] + (1.0 - alpha) * p[n]

return x

if __name__ == '__main__':
iter = 5
G = nx.Graph()

# Add 7 nodes
G.add_nodes_from(range(1,8))

# Add 6 edges
G.add_edges_from([(1,2), (3,1), (3,2), (3,4), (5,4), (6,4)])

# Compute hubs and authorities normalized values using hits
h, a = hits(G, iter=iter)

print 'HITS Algorithm ({} iterations)'.format(iter)
print '============='
print 'Hubs values = {}'.format(h)
print 'Authorities values = {}'.format(a)
print ''

# Compute pagerank of each nodes
pr = pagerank(G, iter=iter)

print 'Pagerank Algorithm ({} iterations)'.format(iter)
print '============='
print 'Pagerank values = {}'.format(pr)

\end{lstlisting}



\noindent\makebox[\linewidth]{\rule{\textwidth}{0.4pt}}

\section*{Question 10.5}
\begin{spverbatim}
Find a community-based question answering site on the Web and ask two questions, one that is low-quality and one that is high-quality. Describe the answer quality of each question.
\end{spverbatim}

\subsection*{Answer:}
For this assignment, I asked 2 questions on 2 different comunity-based question answering site. For the low-quality question, I asked about \textbf{\textit{`What is the purpose of our life?'}} \footnote{https://answers.yahoo.com/question/index?qid=20161216122515AAPvTIw&page=4} on Yahoo Answers \url{https://answers.yahoo.com/} as can be seen on figure \ref{fig:10_5_q}. For the high-quality question, I asked the question \textbf{\textit{`BUILD-MAX-HEAP running time for array sorted in decreasing order'}} on Stackoverflow \footnote{http://stackoverflow.com/questions/39691923/build-max-heap-running-time-for-array-sorted-in-decreasing-order} as can be seen on figure \ref{fig:10_5_a1}.

\begin{figure}[H]
	\fbox{\includegraphics[scale=0.5]{10_5_q}}
	\centering
	\caption{Low Quality Question I asked on Yahoo Answers}
	\label{fig:10_5_q}
\end{figure}

\begin{figure}[H]
	\fbox{\includegraphics[scale=0.6]{10_5_hi_quality}}
	\centering
	\caption{High Quality Question I asked on Stackoverflow}
	\label{fig:10_5_hi_quality}
\end{figure}

By the time I write this report, I got 37 answers for the low-quality question on Yahoo Answers. Some of the answers can be seen on figure \ref{fig:10_5_a1}. For the high-quality question on Stackoverflow, I only got 2 answers as can be seen on figure \ref{fig:10_5_high_quality_answer}. \newline 
However, for the low-quality question, I also got low-quality answers. This is understandable because for a low-quality question, people tend to post anything that they have in their minds without being afraid of any risks. For example, when I asked `What is the purpose of our life', I got answers like `Who says that there's one?', `To study the paintings of the great Masters, and understand that UFOs are real', and `Drink beer and have a good time'. \newline
Accordingly, despite the low number of answers, I got high-quality answers for the high-quality question. This is understandable because for this type of question, people will think twice (or maybe more) before submitting the answers. They should be able to provide not only answer, but also the explanation why the answer is correct. People will not take the risk of embarassing themselves by saying something irrelevant. For a high-quality answer, not everyone has the ability to provide a justifiable answer and explanation. Hence, we got a lower number of answers for the high-quality question compare to the low-quality question. 

\begin{figure}[H]
	\fbox{\includegraphics[scale=0.6]{10_5_a1}}
	\centering
	\caption{Answers for the low-quality question I asked on Yahoo Answers}
	\label{fig:10_5_a1}
\end{figure}

\begin{figure}[H]
	\fbox{\includegraphics[scale=0.6]{10_5_high_quality_answer}}
	\centering
	\caption{Answers for the high-quality question I asked on Stackoverflow}
	\label{fig:10_5_high_quality_answer}
\end{figure}


\noindent\makebox[\linewidth]{\rule{\textwidth}{0.4pt}}

\section*{Question 8.5}
\begin{spverbatim}
question 3
\end{spverbatim}

\subsection*{Answer}
answer 3

\noindent\makebox[\linewidth]{\rule{\textwidth}{0.4pt}}

\section*{Question 8.7}
\begin{spverbatim}
question 4
\end{spverbatim}

\subsection*{Answer}
answer 4

\medskip

\bibliographystyle{unsrt}%Used BibTeX style is unsrt
\bibliography{biblio}

\end{document}
